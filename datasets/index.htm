
<!DOCTYPE html>
<html lang="en">
    <head>
         <meta charset="UTF-8">
         <title>Rob van der Goot</title>
         <link rel="stylesheet" media="screen and (min-width: 1000px)" href="../css/big.css" />
         <link rel="stylesheet" media="screen and (max-width: 1000px)" href="../css/small.css"/>
        <link rel="icon" href="../favicon.ico" type="image/x-icon" />
     </head>
     <body>
         <div id="wrapper">
             <div id="menu">
                <div id="menuText">
                Menu
                </div>
                 <div id="buttonWrapper">
                     <a class="button" href="../">Home</a>
                     <a class="button" href="../papers/">Papers</a>
                     <a class="button" href="../cv/">CV</a>
                     <a class="button" href="../datasets/">Datasets</a>
                     <!--<a class="button" href="../demos/">Demos</a>-->
                     <a class="button" href="../blog/">Blog</a>
                     <a class="button" href="../thesisProjects/">Thesis Projects</a>
                 </div>
             </div>
             <div id="content">

<p>Below I list all the datasets for which I was involved in the creation.</p>

<h2>Lexical Normalization for Italian</h2>
<p>In collaboration with Alan Ramponi, Tommaso Caselli, Michele Cafagna, Lorenzo De Mattei, we developed a dataset for lexical normalization of italian based on the <a href="https://github.com/UniversalDependencies/UD_Italian-PoSTWITA">PoSTWITA</a> data. This dataset contains both random tweets and tweets about politics. Two examples from the dataset: </p>

<div class="code">
ma Grillo nn e'quello ke ha fatto a suo tempo il condono
</br>
ma Grillo non è quello che ha fatto a suo tempo il condono
</div>

<div class="code">
a Roma invece è cosí primavera che sembra gia giov . 
</br>
a Roma invece è così primavera che sembra già giovedì . 
</div>

<p>This dataset is available at <a href="https://bitbucket.org/robvanderg/normit">https://bitbucket.org/robvanderg/normit</a></p>

<h2>Normalization categories</h2>
<p>This dataset accommodates for in-depth evaluation of a normalization model
(e.g. MoNoise),  it is based on a taxonomy containing different normalization
actions. A Twitter corpus is annotated by 2 annotators (thanks Rik!) and is <a
href="https://bitbucket.org/robvanderg/normtax">publicly available</a>.</p>


<h2>MoNoise Treebank</h2>
<p>For this treebank I have used data from datasets commonly used for research:
the <a href="">LexNorm</a> dataset and the <a href="">Owoputi</a> dataset. From
these dataset I extracted all tweets which were still available in December
2018, to ensure that the non-tokenized version was also available. I used the
normalization and POS annotation from <a
href="https://www.aclweb.org/anthology/P11-1038/">[1]</a> <a
href="https://www.aclweb.org/anthology/N13-1039/">[2]</a> <a
href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewFile/10839/10838">[3]</a>.
The POS tags were automatically converted to UPOS and corrected; dependency annotation
is added from scratch with help from Gosse Bouma (thanks!). Predicted
normalization is added by <a href="../monoise/">MoNoise</a>. Finally, with help
from Wessel Reijngoud, a layer of normalization categories, as described above,
is added. The final annotation with all layers looks like:</p> 

<img width="800" style="max-width: 93%;" src="../pics/depTree.png"/>

<p>
This dataset is <a href="https://bitbucket.org/robvanderg/taxeval">available</a> in conll format with all remaining layers in the `misc' column.
</p>

<h2>Shared Tasks</h2>
<p>Shared tasks are indisputably drivers of progress and interest for problems in NLP. To qualify some of the characteristics and potential problems, we <a href="http://bitbucket.org/robvanderg/sharing">annotated</a> a random set of shared tasks by hand. The process is described in the paper <a href="../doc/sharing.pdf">Sharing is Caring: The Future of Shared Tasks</a>.</p>


<h2>Human Judgement for Gender Identification</h2>
<p>Detecting gender based on text is a classic authorship profiling task. Previous work showed that humans can do a reasonable job on this task, being correct in approximately 80% of the cases. Automatic methods seem to reach similar performance if there is enough (similar) training data available (<a href="http://aabeta.herokuapp.com/#/profiling">try it</a>). However, how well can humans and machines perform for this task if they do not speak the language of the utterances? To answer these questions, we annotated a Portugese dataset by native speakers of Dutch and a Dutch dataset annotated by French people. The dataset is available <a href="https://github.com/bplank/bleaching-text">here</a> and the paper with more details <a href="https://www.aclweb.org/anthology/P18-2061.pdf">here</a>.</p>


            </div>
        </div>
    </body>
</html>

