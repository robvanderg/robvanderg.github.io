
<!DOCTYPE html>
<html lang="en">
    <head>
         <meta charset="UTF-8">
         <title>Rob van der Goot</title>
         <link rel="stylesheet" media="screen and (min-width: 1000px)" href="../css/big.css" />
         <link rel="stylesheet" media="screen and (max-width: 1000px)" href="../css/small.css"/>
        <link rel="icon" href="../favicon.ico" type="image/x-icon" />
     </head>
     <body>
         <div id="wrapper">
             <div id="menu">
                <div id="menuText">
                Menu
                </div>
                 <div id="buttonWrapper">
                     <a class="button" href="../">Home</a>
                     <a class="button" href="../papers/">Papers</a>
                     <a class="button" href="../cv/">CV</a>
                     <!--<a class="button" href="../datasets/">Datasets</a>-->
                     <!--<a class="button" href="../demos/">Demos</a>-->
                     <a class="button" href="../blog/">Blog</a>
                     <a class="button" href="../thesisProjects/">Thesis Projects</a>
                 </div>
             </div>
             <div id="content">

<h2>Thesis project topics.</h2>
<p> Contact me if you are interested in any of these (robv@itu.dk).</p>

<h3>Tokenization of social media data</h3>
<p>In many NLP benchmarks, tokenized texts are assumed as input to our models. For standard domains, tokenization can be considered a solved problem, however, for social media text tokenization is non-trivial. The goal of this project is to create a multi-lingual corpus and model for this task. Steps include:</p>

<ul>
    <li>Finding the original utterances of <a href="http://noisy-text.github.io/2021/multi-lexnorm.html">Multi-LexNorm</a></li>
    <li>Create a gold standard dataset based on the original and the tokenized data.</li>
    <li>Evaluate existing tokenizers and train your own.</li>
</ul>

Some related work:

<ul>
    <li><a href="https://www.aclweb.org/anthology/Q18-1030.pdf">Universal Word Segmentation: Implementation and Interpretation</a></li>
    <li><a href="https://github.com/myleott/ark-twokenize-py/blob/master/twokenize.py">twokenize</a></li>
    <li><a href="https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer">nltk.TweetTokenizer</a></li>
</ul>

<h3>Language identification for many languages</h3>

<p>Language identification is a standard NLP task, which is often considered to be solved. However, most current classifiers only support around 100 languages, or are not publicly available. This project makes use of the <a href="http://www.cs.cmu.edu/~ralf/langid.html">LTI LangID Corpus</a>, and asks the question: how do modern neural network approaches compare to simple character-based classifiers for language classification (with &#62;1300 languages). Relevant previous work:</p>

<ul>
    <li><a href="https://www.aclweb.org/anthology/D14-1069.pdf">Non-linear Mapping for Improved Identification of 1300+ Languages</a></li>
    <li><a href="https://www.aclweb.org/anthology/D18-1030.pdf">A Fast, Compact, Accurate Model for Language Identification of Codemixed Text</a></li>
    <li><a href="https://www.aclweb.org/anthology/2020.coling-main.579.pdf">Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus</a></li>
</ul>


<h3>The effect of translationese on slot and intent detection</h3>
<p>The tasks of slot and intent detection is a crucial component of digital assisnents. Intent detection aims to find the goal of an utterance, and slot detection finds relevant entities. An example of this task:</p>

<table>
  <tr>
    <th>Add</th>
    <th>reminder </th>
<th>to </th>
<th>swim </th>
<th>at </th>
<th>11am </th>
<th>tomorrow</th>
    </tr>
    <tr>
    <th></th>
    <th> </th>
<th> </th>
<th>B-TODO </th>
<th> </th>
<th>B-DATETIME </th>
<th>I-DATETIME</th>
</tr>
<tr>
<th>Intent</th>
<th>add-reminder</th>
    <th></th>
    <th></th>
    <th></th>
    <th></th>
    <th></th>
</tr>
</table>

<p>Recently, two big multi-lingual datasets have been introduced (multiAtis, and xSID). However, these datasets consist of data translated from English. Translationese is known to be different from spontaneous language. This project aims to estimate the effect of this difference. By generating a small sample of native non-English data (for example Danish), and evaluating this against the xSID data.</p>

<p>Here you can find the xSID paper, and an investigation on translationese for machine translation evaluation:</p>
<ul>
    <li><a href="https://aclanthology.org/2021.naacl-main.197.pdf">From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding</a></li>
    <li><a href="https://aclanthology.org/W19-5208.pdf">The Effect of Translationese in Machine Translation Test Sets</a></li>
</ul>


<h3>Multi-lingual lexical normalization</h3>
<p>Lexical normalization is the task of converting social media language to its canoical equivalent. A traditional machine learning still holds the state-of-the-art (as opposed to deep learning), and most approaches are language specific. However, recently a dataset with 12 languages was introduced. More information on:</p>

<ul>
    <li>Website for dataset: <a href="http://noisy-text.github.io/2021/multi-lexnorm.html">Multi-LexNorm</a></li>
    <li><a href="https://www.aclweb.org/anthology/P19-3032.pdf">MoNoise: A Multi-lingual and Easy-to-use Lexical Normalization Tool</a></li>
    <li><a href="https://www.aclweb.org/anthology/W15-4319.pdf">Shared Tasks of the 2015 Workshop on Noisy User-generated Text: Twitter Lexical Normalization and Named Entity Recognition</a></li>
</ul>

<h3>Dependency parsing of Danish social media data</h3>
Dependency parsing is the task of finding the syntactic relations between words in a sentence. <a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf">Chapter 14</a> of the Speech and Language Processing book contains a nice introduction to this topic. Many different languages and domains have been covered by the recent <a href="https://universaldependencies.org/">Universal Dependencies project</a>. However, for language types not covered performance is generally lower. Recently, we have collected some non-canonical data samples: <a href="https://www.aclweb.org/anthology/2020.coling-main.583.pdf">DaN+</a>, for which it is uncertain how well current methods would perform.

The goal of this project would be to annotate a small sample of Danish social media data to evaluate parsers. Then, a variety of approaches of adapting the parser could be studied, including the ones mentioned below:

<ul>
    <li><a href="https://www.aclweb.org/anthology/2021.adaptnlp-1.6.pdf">Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data</a></li>
    <li><a href="https://www.aclweb.org/anthology/D19-6118.pdf">Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study</a></li>
    <li><a href="https://www.aclweb.org/anthology/D19-1102.pdf">A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages</a></li>
    <li><a href="https://www.aclweb.org/anthology/W19-7713.pdf">How to Parse Low-Resource Languages: Cross-Lingual Parsing, Target Language Annotation, or Both?</a></li>
    <li><a href="https://www.aclweb.org/anthology/D18-1542.pdf">Modeling Input Uncertainty in Neural Network Dependency Parsing</a></li>
</ul>

<h3>Active learning for POS tagging</h3>
<p>POS tagging is the task of classifying words into their syntactic category:</p>

<table>
  <tr>
    <th>I</th>
    <th>see </th>
    <th>the</th>
    <th>light</th>
  </tr>
  <tr>
    <td>PRON</td>
    <td>VERB</td>
    <td>DET</td>
    <td>NOUN</td>
  </tr>
</table>

<p>Current POS tagger are usually supervised, which means they rely on human-annotated training data. This data commonly exists of thousands of sentences. To make this process less costly, one can select a more informative sample of words to rely on, and instead only annotate this subsample. Previous work (see below) has shown that competetive performance can be obtained with as little as 400 words on English news data. However, it is unclear how this transfers to other languages/domains. In this project, the first step is to evaluate the existing method on a larger sample (i.e. the <a href="https://universaldependencies.org/">Universal Dependencies dataset</a>), followed by possible improvements to the model.</p>

<p> Related reading:</p>
<ul>
    <li><a href="https://www.aclweb.org/anthology/W15-1511.pdf">Simple Semi-Supervised POS Tagging</a></li>
</ul>

<h3>Unsupervised code-switch detection</h3>
<p>Code-switching is the phenomenon of switching to another language within one utterance. Many previous approaches have been evaluated for a variety of language pairs; however, they are all trained on annotated code-switched data.</p>

<p>To increase the usefulness of such a code-switch detector, the idea is to train a system based on two monolingual datasets to predict language labels on the word level. An example of the desired output is shown below:</p>

<table>
  <tr>
    <th>@friend</th>
    <th>u </th>
    <th>perform</th>
    <th>besop</th>
    <th>apa</th>
    <th>tudey</th>
    <th>?</th>
  </tr>
  <tr>
    <td>un</td>
    <td>en</td>
    <td>en</td>
    <td>id</td>
    <td>id</td>
    <td>en</td>
    <td>?</td>
  </tr>
</table>

<p> Related reading:</p>
<ul>
    <li><a href="https://www.atala.org/sites/default/files/2.Das-TAL54-3.pdf">Code-Mixing in Social Media Text</a></li>
    <li><a href="https://www.aclweb.org/anthology/W14-3907.pdf">Overview for the First Shared Task on Language Identification in Code-Switched Data</a></li>
    <li><a href="https://www.aclweb.org/anthology/W16-5805.pdf">Overview for the Second Shared Task on Language Identification in Code-Switched Data</a></li>
    <li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/Overview.pdf">Overview of FIRE-2015 Shared Task on Mixed Script Information Retrieval</a></li>
    <li><a href="http://ceur-ws.org/Vol-1737/T3-1.pdf">Overview of the Mixed Script Information Retrieval (MSIR) at FIRE-2016</a></li>
    <li><a href="https://www.aclweb.org/anthology/D18-1030.pdf">A Fast, Compact, Accurate Model for Language Identification of Codemixed Text</a></li>
</ul>

<h3>Conversion of NLP tasks to sequence labeling tasks</h3>
<p>Because of the continually increasing power of sequence labelers,
competetive performance for complex tasks can be gained by simplifying tasks to
sequence labeling problems. This lead to efficient and accurate NLP models. The
main setup is: 1) find a task (I have a couple in mind already of course),
2) convert this a sequence labeling problem 3) train a sequence labeler for this
conversion, 4) then convert the sequence back to the original task and
evaluate. This is mainly an algorithmic project, as existing sequence labelers can be used.</p>

<p>Succesfull examples on some NLP tasks:</p>
<ul>
    <li><a href="../doc/beesl.pdf">Biomedical Event Extraction as Sequence Labeling</a></li>
    <li><a href="https://www.aclweb.org/anthology/2020.acl-main.557.pdf">Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference</a></li>
    <li><a href="https://www.aclweb.org/anthology/N19-1077.pdf">Viable Dependency Parsing as Sequence Labeling</a></li>
</ul>


<h3>Strategies for Morphological Tagging</h3>
<p>Morphological tagging is the task of assigning labels to a sequence of tokens that describe them morphologically.  
This means that one word can have 0-n labels. There has been a variety of architectures proposed to solve this task, 
however it is unclear which method works best in which situation. </p>

<p>In this project you can make use of the <a
href="https://universaldependencies.org/">Universal Dependencies</a> data,
which has annotation for morphological tags for many languages. You can use the
<a href="https://machamp-nlp.github.io/">MaChAmp</a> toolkit, or implement a
BiLSTM tagger yourself, and evaluate at least the three most common
strategies:</p>

<ul>
    <li>Predict the concatenation of the tags as one label (same as POS tagging, but with more labels)</li>
    <li>Predict morphological tags as a sequence (like machine translation)</li>
    <li>View the task as a multilabel prediction problem (Get a probability for each label, and set a cutoff threshold)</li>
</ul>

<p>Related reading:</p>
<ul>
    <li><a href="https://aclanthology.org/W19-4226/">The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection</a></li>
    <li><a href="https://aclanthology.org/W19-4206">Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.</a></li>
</ul>

<h3>Can humans recognize NLP domains?</h3>
<p>In natural language processing, we often assume that the origin/platform of
a text is its domain. These "domains" are then used to evaluate cross-domain
performance. However, in many cases the domain or genre of a text might not be
so clear-cut.  Even tough theoretical frameworks have been proposed, it is
unclear whether domain properties can be identified from an utterance directly.
In this project, we ask: How well can humans identify which domain a sentence
is taken from?, and how does this compare to automatic models?, existing
datasets such as the <a
href="https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf">AG
News corpus</a> or the <a href="https://universaldependencies.org/">Universal
Dependencies</a> data can be used for this project.</p>

<p>Related reading:</p>
<ul>
    <li><a href="https://bplank.github.io/papers/konvens2016.pdf">What to do about non-standard (or non-canonical) language in NLP</a></li>
</ul>


<!--
<h3>Dialogue acts classification for social media data</h3>
<p>Dialogue acts classify the intended goal of a textual utterance. Dialogue acts have mostly been studied in the context of telephone conversations. However, when sharing utterances on social media, people generally also have an intended goal (perhaps not always, so a MISC column should be considered).</p>

<p>Some resources on Dialogue Acts:</p>
<ul>
    <li><a href="https://web.stanford.edu/~jurafsky/ws97/manual.august1.html">Definition of the task</a></li>
    <li>John Langshaw Austin. How to Do Things with Words</li>
    <li><a href="http://faculty.nps.edu/cmartell/NPSChat.htm">NPSChat corpus</a></li>
</ul>-->

<h3>Effect of sociodemographic factors on language use</h3>
Recent work has shown that including the origin of a text instance can improve
performance on NLP tasks. However, it is unclear which specific sociodemographic
attributes correlate with language use. Recent efforts on annotation of social
media data could give us more insights.

<ul>
    <li><a href="https://www.aclweb.org/anthology/P19-1339.pdf">Women’s Syntactic Resilience and Men’s Grammatical Luck:
Gender-Bias in Part-of-Speech Tagging and Dependency Parsing</a></li>
    <li><a href="https://journals.sagepub.com/doi/pdf/10.1177/007242030002005">Gender Differences in English Syntax</a></li>
    <li><a href="https://www.aclweb.org/anthology/K15-1011.pdf">Cross-lingual syntactic variation over age and gender</a></li>
</ul>

<!--<h3>Annotation tool for standard NLP formats</h3>
In NLP tab-seperated files are commonly used (like in the 
<a href="https://universaldependencies.org/">UD data</a>).
To get the ground truth for which we can train machine learning models, we
first let humans annotate data. Many annotation tools exist for NLP tasks;
however, most of them use their own data formats. Furthermore, changing the
original text (for example to seperate words from punctuation) is generally not
supported.  Because of this, it is cumbersome to use these tools, as you have
to keep converting the data.  Novelties for this tool would be:

<img src="../pics/annotation.jpg" style="float: right; margin-right: 1%; margin-top:50px; width:300px;" alt="me" >

<ul>
    <li>Browser-based</li>
    <li>Directly use exising NLP standard formats</li>
    <li>Visualize the annotation and the raw text file simultaneously</li>
    <li>Support for span-label annotation and sentence level classification (other tasks can be included later)</li>
    <li>Support left-to-right languages</li>
    <li>(It could even include a prediction model to save time for the annotators)</li>
</ul>

Existing tools:
<ul>
    <li><a href="https://brat.nlplab.org/">BRAT</a></li>
    <li><a href="https://github.com/doccano/doccano">Doccano</a></li>
    <li><a href="https://prodi.gy/">Prodigy</a></li>
</ul>
-->


<!--
<h3>Machine translation of social media data</h3>
<p>While performance of machine translation keeps increasing, the current models do not perform well on non-standard (i.e. social media) text. One solution to this problem would be to transform this data a `normal' form (e.g. by using <a href="../monoise">MoNoise</a>) before translating it.</p>

<p>Relevant reading:</p>
<ul>
    <li><a href="https://www.aclweb.org/anthology/D18-1050.pdf">MTNT: a testbed for MT of noisy text</a></li>
    <li><a href="https://www.aclweb.org/anthology/N13-1050.pdf">A Beam-Search Decoder for Normalization of Social Media Text with Application to Machine Translation</a></li>
    <li><a href="Microblogs as Parallel Corpora">Microblogs as Parallel Corpora</a></li>
    <li><a href="https://www.aclweb.org/anthology/W19-5303.pdf">Findings of the First Shared Task on Machine Translation Robustness</a>
</ul>-->


<!--<h3>Low-resource dependency parsing</h3>
<p>Dependency parsing is the task of finding the syntactic relations between words in a sentence. For more information I refer to the <a href="https://universaldependencies.org/">UD page</a>, which contains data annotated for this task for over 70 languages. </p>

<p>Very high scores have been obtained for this task (&gt; 95%), however this is all done by supervised parsers, which are trained on large amounts of annotated data. I am generally interested in doing natural language processing for less-resourced situations (languages/domains). Yes, this topic is less concrete as the previous one, contact me for more concrete ideas. Some interesting recent work:</p>

<ul>
    <li><a href="https://www.aclweb.org/anthology/D19-1102/">A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages</a></li>
    <li><a href="https://www.aclweb.org/anthology/D19-6118/">Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study</a></li>
    <li><a href="https://www.aclweb.org/anthology/W19-7713/">How to Parse Low-Resource Languages: Cross-Lingual Parsing, Target Language Annotation, or Both?</a></li>
</ul>-->

<!--
<h3>Contextualized representations for word graphs</h3>
<p><b>Be aware, this is a complex topic</b></p>
<p>Use of contextualized embeddings (e.g. BERT) for word graphs. This can be used in multiple cases:</p>

<ul>
    <li>Simplified (number of nodes are known): <a href="">http://robvandergoot.com/doc/acl17.pdf</a></li>
    <li>Could also be more advanced, when parsing full word-graphs. Then insertion/deletion/splitting of words can be included.</li>
</ul>
-->


            </div>
        </div>
    </body>
</html>

