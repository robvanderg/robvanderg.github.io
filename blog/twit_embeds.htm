
<!DOCTYPE html>
<html lang="en">
    <head>
         <meta charset="UTF-8">
         <title>Rob van der Goot</title>
         <link rel="stylesheet" media="screen and (min-width: 1000px)" href="../css/big.css" />
         <link rel="stylesheet" media="screen and (max-width: 1000px)" href="../css/small.css"/>
        <link rel="icon" href="../favicon.ico" type="image/x-icon" />
     </head>
     <body>
         <div id="wrapper">
             <div id="menu">
                <div id="menuText">
                Menu
                </div>
                 <div id="buttonWrapper">
                     <a class="button" href="../">Home</a>
                     <a class="button" href="../papers/">Papers</a>
                     <a class="button" href="../cv/">CV</a>
                     <a class="button" href="../datasets/">Datasets</a>
                     <a class="button" href="../demos/">Demos</a>
                     <a class="button" href="../blog/">blog</a>
                     <a class="button" href="../thesisProjects/">Thesis Projects</a>
                 </div>
             </div>
             <div id="content">

<h2>Multi-lingual Twitter word embeddings</h2>

<p>Since I have started to work on Twitter data, word embeddings have proven to be very useful. In the last two years, word embeddings have mostly been replaced by contextualized transformer based embeddings. For multi-lingual contextual twitter embeddings I refer to <a href="https://arxiv.org/pdf/2104.12250.pdf">Barbieri et al.</a> (also available on <a href="https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base">huggingface</a>). However, there are still many cases where word embeddings are preferable, because of their efficiency, or because they still result in better performance. Yes, they sometimes are still not outperformed, for example for the task of lexical normalization (<a href="https://www.aclweb.org/anthology/P19-3032.pdf">SOTA</a>).</p>

<p>I have prepared word embeddings for all languages included in the <a href="http://noisy-text.github.io/2021/multi-lexnorm.html">Multi-LexNorm shared task</a>. The procedure was as follows:</p>

<ul>
    <li>Download sample of tweets between 2012-2020 from <a href="https://archive.org/details/twitterstream">archive.org</a></li>
    <li>Used the <a href="https://fasttext.cc/docs/en/language-identification.html">Fasttext language classifier</a> for language identification. Empirical results looked much better as the Twitter provided language labels.</li>
    <li>For the code-switched language pairs, I have simply concatenated both mono-lingual datasets, as it is non-trivial to filter for code-switched data.</li>
    <li>Cleaned usernames and url's to make the vocabulary smaller, and anonymize. Used the following command:</li>
<div class="code">sed -r 's/@[^ ][^ ]*/<USER>/g' | sed -r 's/(http[s]?:\/[^ ]*|www\.[^ ]*)/<URL>/g' </div>
    <li>Removed duplicates with (note that this stores intermediate results in /dev/shm, which should be quite large):</li>
<div class="code">sort -T /dev/shm | uniq</div>
    <li>trained word2vec with the following settings:</li>
<div class="code">./word2vec/word2vec -train nl.txt  -output id.bin -size 400 -window 5 -cbow 0 -binary 1 -threads 45</div>
</ul>

<p>You might notice that I did not do any tokenization. This is not because I forgot. This is done because any consistent errors in tokenization would lead to specific words being excluded from the vocabulary.</p>

<p>The sizes of the files, and the number of characters, words and tweets are:</p>
<table>
    <tr>
        <td>Lang.</td>
        <td>Chars</td>
        <td>Words.</td>
        <td>Tweets</td>
        <td>Size</td>
    </tr>
    <tr>
        <td>da</td>
        <td>159,067,945</td>
        <td>26,410,783</td>
        <td>2,939,931</td>
        <td>152M</td>
    </tr>
    <tr>
        <td>de</td>
        <td>4,017,217,589</td>
        <td>602,955,881</td>
        <td>72,054,802</td>
        <td>3.8G</td>
    </tr>
    <tr>
        <td>en</td>
        <td>183,774,280,286</td>
        <td>31,463,897,778</td>
        <td>2,526,522,685</td>
        <td>172G</td>
    </tr>
    <tr>
        <td>es</td>
        <td>75,656,330,294</td>
        <td>9,602,044,523</td>
        <td>765,704,695</td>
        <td>53G</td>
    </tr>
    <tr>
        <td>hr</td>
        <td>99,558,448</td>
        <td>16,352,437</td>
        <td>2,007,553</td>
        <td>95M</td>
    </tr>
    <tr>
        <td>id</td>
        <td>15,355,311,741</td>
        <td>2,479,391,528</td>
        <td>196,348,197</td>
        <td>15G</td>
    </tr>
    <tr>
        <td>iden</td>
        <td>199,129,592,027</td>
        <td>33,943,289,306</td>
        <td>2,722,870,882</td>
        <td>186G</td>
    </tr>
    <tr>
        <td>it</td>
        <td>4,082,095,927</td>
        <td>650,557,697</td>
        <td>64,662,978</td>
        <td>3.9G</td>
    </tr>
    <tr>
        <td>nl</td>
        <td>2,842,694,893</td>
        <td>480,387,036</td>
        <td>45,942,710</td>
        <td>2.7G</td>
    </tr>
    <tr>
        <td>sl</td>
        <td>192,472,502</td>
        <td>22,977,241</td>
        <td>3,577,682</td>
        <td>184M</td>
    </tr>
    <tr>
        <td>sr</td>
        <td>403,058,101</td>
        <td>58,043,354</td>
        <td>5,903,680</td>
        <td>385M</td>
    </tr>
    <tr>
        <td>tr</td>
        <td>11,400,083,503</td>
        <td>1,461,947,731</td>
        <td>133,557,943</td>
        <td>11G</td>
    </tr>
    <tr>
        <td>trde</td>
        <td>15,417,301,092</td>
        <td>2,064,903,612</td>
        <td>205,612,745</td>
        <td>15G</td>
    </tr>
</table>

<p>The results of this procedure are hosted on: <a href="http://www.itu.dk/people/robv/data/embeds/">http://www.itu.dk/people/robv/data/embeds/</a></p>

<p>Besides the embeddings, I have also counter uni- and bi-gram frequencies on the same data. I have used a minimum frequency of 3. Results of this are shared on: <a href="http://www.itu.dk/people/robv/data/ngrams/">http://www.itu.dk/people/robv/data/ngrams/</a></p>

            </div>
        </div>
    </body>
</html>

