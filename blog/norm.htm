
<!DOCTYPE html>
<html lang="en">
    <head>
         <meta charset="UTF-8">
         <title>Rob van der Goot</title>
         <link rel="stylesheet" media="screen and (min-width: 1000px)" href="../css/big.css" />
         <link rel="stylesheet" media="screen and (max-width: 1000px)" href="../css/small.css"/>
        <link rel="icon" href="../favicon.ico" type="image/x-icon" />
     </head>
     <body>
         <div id="wrapper">
             <div id="menu">
                <div id="menuText">
                Menu
                </div>
                 <div id="buttonWrapper">
                     <a class="button" href="../">Home</a>
                     <a class="button" href="../papers/">Papers</a>
                     <a class="button" href="../cv/">CV</a>
                     <a class="button" href="../datasets/">Datasets</a>
                     <a class="button" href="../demos/">Demos</a>
                     <a class="button" href="../blog/">Blog</a>
                     <a class="button" href="../thesisProjects/">Thesis Projects</a>
                 </div>
             </div>
             <div id="content">

<h2>Normalization datasets</h2>
<p>In the MultiLexNorm shared task (WNUT 2021), we made a first attempt at
homogenising multiple lexical normalization datasets in a variety of languages
into one standard.  This project was started to improve the evaluation and
comparison of existing lexical normalization models, as well as pushing the
focus to a larger variety of languages. We defined lexical normalization as the
task of "transforming an utterance into its standard form, word by word,
including both one-to-many (1-n) and many-to-one (n-1) replacements." An
example of an utterance annotated for this task would be:</p>

<table>
  <tr>
    <th class="tg-0lax">most</th>
    <th class="tg-0lax">social</th>
    <th class="tg-0lax">pple</th>
    <th class="tg-0lax">r</th>
    <th class="tg-0lax">troublsome</th>
  </tr>
  <tr>
    <td class="tg-0lax">most</td>
    <td class="tg-0lax">social</td>
    <td class="tg-0lax">people</td>
    <td class="tg-0lax">are</td>
    <td class="tg-0lax">troublesome</td>
  </tr>
</table>

<p>More examples and information about MultiLexNorm can be found on the <a
href="http://noisy-text.github.io/2021/multi-lexnorm.html">task website</a> and
<a href="http://noisy-text.github.io/2021/multi-lexnorm.html/">overview
paper</a>.</p>

<p>On this page, I collect references to datasets that were not included in
MultiLexNorm for a variety of reasons, some of these are word-based, not
publicly available/sharable, they include translation/transcription, or I only
found out about them after the shared task. Hopefully, the MultiLexNorm
benchmark will be expanded in the future with more varied languages. Note that
I focus on social media datasets here, there are also historical and medical
datasets for the lexical normalization task.</p>






            </div>
        </div>
    </body>
</html>

